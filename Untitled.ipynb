{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    },
    "colab": {
      "name": "Untitled.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/firdarinoa/bangkit-assignment5/blob/Deni%2FAssignment/Untitled.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3O3bP1OQTD8p",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow import feature_column\n",
        "from tensorflow.keras import layers\n",
        "from sklearn.model_selection import train_test_split"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tJyRUNn2TD81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cq__CmvHTD86",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#import data\n",
        "dataframe1 = pd.read_csv('Dataset_spine.csv')\n",
        "\n",
        "# Convert last column to be a table and drop it.\n",
        "attributes = dataframe1.iloc[5:17][\"Unnamed: 13\"]\n",
        "dataframe1.drop([\"Unnamed: 13\"], axis=1,inplace=True)\n",
        "\n",
        "#convert label data (class_att)\n",
        "dataframe1.Class_att = [1 if i ==\"Abnormal\" else 0 for i in dataframe1.Class_att]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p9cFTm3CTD89",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#EDA ideas\n",
        "#EDA: sebelum di normalisasi-> buat plot histogramnnya dulu liat skew apa enggak"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_hkW_POTTD9B",
        "colab_type": "code",
        "colab": {},
        "outputId": "8d06b2c2-1d39-4195-f983-4c3dc1c3afe7"
      },
      "source": [
        "k=len(dataframe1.Class_att)\n",
        "i=0\n",
        "for j in range(len(dataframe1.Class_att)):\n",
        "    if dataframe1.Class_att[j]==1:\n",
        "        i+=1\n",
        "print(\"Amount of Data     :\", k)\n",
        "print(\"Amount of Abnormal :\", i)\n",
        "print(\"Amount of Normal   :\", (k-i))"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Amount of Data     : 310\n",
            "Amount of Abnormal : 210\n",
            "Amount of Normal   : 100\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7mPXXee6TD9G",
        "colab_type": "code",
        "colab": {},
        "outputId": "e1548d15-d50d-430f-9c18-30e55281916e"
      },
      "source": [
        "dataframe1_mean = dataframe1.mean()\n",
        "dataframe1_std = dataframe1.std()\n",
        "dataframe = (dataframe1-dataframe1_mean)/dataframe1_std\n",
        "\n",
        "for i in range(len(dataframe)):\n",
        "    if dataframe.Class_att[i]>=0:\n",
        "        dataframe.Class_att[i]=1\n",
        "    elif dataframe.Class_att[i]<0:\n",
        "        dataframe.Class_att[i]=0\n",
        "\n",
        "dataframe"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Col1</th>\n",
              "      <th>Col2</th>\n",
              "      <th>Col3</th>\n",
              "      <th>Col4</th>\n",
              "      <th>Col5</th>\n",
              "      <th>Col6</th>\n",
              "      <th>Col7</th>\n",
              "      <th>Col8</th>\n",
              "      <th>Col9</th>\n",
              "      <th>Col10</th>\n",
              "      <th>Col11</th>\n",
              "      <th>Col12</th>\n",
              "      <th>Class_att</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.146849</td>\n",
              "      <td>0.500559</td>\n",
              "      <td>-0.664103</td>\n",
              "      <td>-0.184652</td>\n",
              "      <td>-1.445310</td>\n",
              "      <td>-0.706916</td>\n",
              "      <td>0.950094</td>\n",
              "      <td>-1.013427</td>\n",
              "      <td>0.433592</td>\n",
              "      <td>1.165245</td>\n",
              "      <td>-1.194656</td>\n",
              "      <td>1.709604</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>-1.243853</td>\n",
              "      <td>-0.747560</td>\n",
              "      <td>-1.450655</td>\n",
              "      <td>-1.039839</td>\n",
              "      <td>-0.263958</td>\n",
              "      <td>-0.578621</td>\n",
              "      <td>-0.202226</td>\n",
              "      <td>-0.976237</td>\n",
              "      <td>1.314166</td>\n",
              "      <td>1.676840</td>\n",
              "      <td>-0.938807</td>\n",
              "      <td>-0.912466</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.483588</td>\n",
              "      <td>0.467177</td>\n",
              "      <td>-0.099102</td>\n",
              "      <td>0.272642</td>\n",
              "      <td>-0.896237</td>\n",
              "      <td>-0.794137</td>\n",
              "      <td>0.006683</td>\n",
              "      <td>0.638095</td>\n",
              "      <td>1.300577</td>\n",
              "      <td>1.633329</td>\n",
              "      <td>-1.225197</td>\n",
              "      <td>-0.614693</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.510564</td>\n",
              "      <td>0.710414</td>\n",
              "      <td>-0.410675</td>\n",
              "      <td>0.125925</td>\n",
              "      <td>-1.205354</td>\n",
              "      <td>-0.401639</td>\n",
              "      <td>-0.362627</td>\n",
              "      <td>0.259135</td>\n",
              "      <td>-0.105042</td>\n",
              "      <td>-0.175873</td>\n",
              "      <td>-1.342849</td>\n",
              "      <td>-0.651935</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>-0.625636</td>\n",
              "      <td>-0.788418</td>\n",
              "      <td>-1.272687</td>\n",
              "      <td>-0.215527</td>\n",
              "      <td>-0.732271</td>\n",
              "      <td>-0.489315</td>\n",
              "      <td>0.246272</td>\n",
              "      <td>1.640442</td>\n",
              "      <td>0.850098</td>\n",
              "      <td>-1.057956</td>\n",
              "      <td>-0.190194</td>\n",
              "      <td>-0.069746</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>305</th>\n",
              "      <td>-0.730605</td>\n",
              "      <td>-0.392287</td>\n",
              "      <td>-0.858622</td>\n",
              "      <td>-0.645674</td>\n",
              "      <td>-0.035412</td>\n",
              "      <td>-0.813176</td>\n",
              "      <td>-1.201019</td>\n",
              "      <td>-1.560084</td>\n",
              "      <td>0.495303</td>\n",
              "      <td>-1.180758</td>\n",
              "      <td>-0.137072</td>\n",
              "      <td>-1.349094</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>306</th>\n",
              "      <td>-0.380582</td>\n",
              "      <td>0.317603</td>\n",
              "      <td>-1.224012</td>\n",
              "      <td>-0.725509</td>\n",
              "      <td>-0.266930</td>\n",
              "      <td>-0.711352</td>\n",
              "      <td>-1.487353</td>\n",
              "      <td>-0.245725</td>\n",
              "      <td>1.509742</td>\n",
              "      <td>-1.674878</td>\n",
              "      <td>1.641393</td>\n",
              "      <td>1.743765</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>307</th>\n",
              "      <td>0.055112</td>\n",
              "      <td>0.514786</td>\n",
              "      <td>-0.310475</td>\n",
              "      <td>-0.313057</td>\n",
              "      <td>0.581952</td>\n",
              "      <td>-0.772240</td>\n",
              "      <td>-1.371333</td>\n",
              "      <td>-0.592126</td>\n",
              "      <td>0.144715</td>\n",
              "      <td>-1.049885</td>\n",
              "      <td>1.441044</td>\n",
              "      <td>-0.691913</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>308</th>\n",
              "      <td>-0.884393</td>\n",
              "      <td>-0.884230</td>\n",
              "      <td>-0.557711</td>\n",
              "      <td>-0.476358</td>\n",
              "      <td>0.046945</td>\n",
              "      <td>-0.694425</td>\n",
              "      <td>-1.097772</td>\n",
              "      <td>-0.762565</td>\n",
              "      <td>0.890748</td>\n",
              "      <td>-0.751434</td>\n",
              "      <td>1.621227</td>\n",
              "      <td>0.772516</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>309</th>\n",
              "      <td>-1.546427</td>\n",
              "      <td>-1.245845</td>\n",
              "      <td>-0.824062</td>\n",
              "      <td>-1.056848</td>\n",
              "      <td>0.452386</td>\n",
              "      <td>-0.705448</td>\n",
              "      <td>0.705158</td>\n",
              "      <td>-0.224439</td>\n",
              "      <td>1.362406</td>\n",
              "      <td>0.620743</td>\n",
              "      <td>1.295329</td>\n",
              "      <td>1.431399</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>310 rows × 13 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         Col1      Col2      Col3      Col4      Col5      Col6      Col7  \\\n",
              "0    0.146849  0.500559 -0.664103 -0.184652 -1.445310 -0.706916  0.950094   \n",
              "1   -1.243853 -0.747560 -1.450655 -1.039839 -0.263958 -0.578621 -0.202226   \n",
              "2    0.483588  0.467177 -0.099102  0.272642 -0.896237 -0.794137  0.006683   \n",
              "3    0.510564  0.710414 -0.410675  0.125925 -1.205354 -0.401639 -0.362627   \n",
              "4   -0.625636 -0.788418 -1.272687 -0.215527 -0.732271 -0.489315  0.246272   \n",
              "..        ...       ...       ...       ...       ...       ...       ...   \n",
              "305 -0.730605 -0.392287 -0.858622 -0.645674 -0.035412 -0.813176 -1.201019   \n",
              "306 -0.380582  0.317603 -1.224012 -0.725509 -0.266930 -0.711352 -1.487353   \n",
              "307  0.055112  0.514786 -0.310475 -0.313057  0.581952 -0.772240 -1.371333   \n",
              "308 -0.884393 -0.884230 -0.557711 -0.476358  0.046945 -0.694425 -1.097772   \n",
              "309 -1.546427 -1.245845 -0.824062 -1.056848  0.452386 -0.705448  0.705158   \n",
              "\n",
              "         Col8      Col9     Col10     Col11     Col12  Class_att  \n",
              "0   -1.013427  0.433592  1.165245 -1.194656  1.709604        1.0  \n",
              "1   -0.976237  1.314166  1.676840 -0.938807 -0.912466        1.0  \n",
              "2    0.638095  1.300577  1.633329 -1.225197 -0.614693        1.0  \n",
              "3    0.259135 -0.105042 -0.175873 -1.342849 -0.651935        1.0  \n",
              "4    1.640442  0.850098 -1.057956 -0.190194 -0.069746        1.0  \n",
              "..        ...       ...       ...       ...       ...        ...  \n",
              "305 -1.560084  0.495303 -1.180758 -0.137072 -1.349094        0.0  \n",
              "306 -0.245725  1.509742 -1.674878  1.641393  1.743765        0.0  \n",
              "307 -0.592126  0.144715 -1.049885  1.441044 -0.691913        0.0  \n",
              "308 -0.762565  0.890748 -0.751434  1.621227  0.772516        0.0  \n",
              "309 -0.224439  1.362406  0.620743  1.295329  1.431399        0.0  \n",
              "\n",
              "[310 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 390
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U4dz-iYTTD9J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# split data jadi train, validation, and split\n",
        "train, test = train_test_split(dataframe, test_size=0.2)\n",
        "train, val = train_test_split(train, test_size=0.2)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Y0hbUjvTD9O",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#create a tf.data dataset from a Pandas Dataframe\n",
        "def df_to_dataset(dataframe, shuffle=True):\n",
        "  dataframe = dataframe.copy()\n",
        "  labels = dataframe.pop('Class_att')\n",
        "  ds = tf.data.Dataset.from_tensor_slices((dict(dataframe), labels))\n",
        "  if shuffle:\n",
        "    ds = ds.shuffle(buffer_size=len(dataframe))\n",
        "  return ds"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mNYk6kUeTD9S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "batch_size = 5 # A small batch sized is used for demonstration purposes\n",
        "train_ds = df_to_dataset(train)\n",
        "val_ds = df_to_dataset(val, shuffle=False)\n",
        "test_ds = df_to_dataset(test, shuffle=False)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9oQ1CUiATD9W",
        "colab_type": "code",
        "colab": {},
        "outputId": "b45d4c82-51da-45a8-cf20-c4b4543d738b"
      },
      "source": [
        "#feature_batch.keys()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['Col1', 'Col2', 'Col3', 'Col4', 'Col5', 'Col6', 'Col7', 'Col8', 'Col9', 'Col10', 'Col11', 'Col12'])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 394
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mDFcCfuUTD9a",
        "colab_type": "code",
        "colab": {},
        "outputId": "01ff9da4-bfa6-49f0-a56c-f9d7424a53d0"
      },
      "source": [
        "#feature_layer"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.feature_column.dense_features_v2.DenseFeatures at 0x252525f26c8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 395
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t4J7U1IdTD9f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for feature_batch, label_batch in train_ds.take(1):\n",
        " # print('Every feature:', list(feature_batch.keys()))\n",
        " # print('A batch of ages:', feature_batch['Col1'])\n",
        " # print('A batch of targets:', label_batch )\n",
        "\n",
        "feature = ['Col1','Col2','Col3','Col4','Col5','Col6']\n",
        "\n",
        "feature_columns = []\n",
        "\n",
        "for i in feature:\n",
        "    feature_columns.append(feature_column.numeric_column(i))\n",
        "\n",
        "#for header in feature_batch.keys() :\n",
        "  #feature_columns.append(feature_column.numeric_column(header))\n",
        "\n",
        "feature_layer = tf.keras.layers.DenseFeatures(feature_columns)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2XZ5__WpTD9k",
        "colab_type": "code",
        "colab": {},
        "outputId": "e381032b-29df-4ddd-9352-6278cb5929c2"
      },
      "source": [
        "#@title Define the functions that create and train a model.\n",
        "def create_model(my_learning_rate, feature_layer, my_metrics):\n",
        "  \"\"\"Create and compile a simple classification model.\"\"\"\n",
        "  # Most simple tf.keras models are sequential.\n",
        "  model = tf.keras.models.Sequential()\n",
        "\n",
        "  # Add the feature layer (the list of features and how they are represented)\n",
        "  # to the model.\n",
        "  model.add(feature_layer)\n",
        "\n",
        "  # Implement L2 regularization in the first hidden layer.\n",
        "  model.add(tf.keras.layers.Dense(units=20, \n",
        "                                  activation='relu',\n",
        "                                  kernel_regularizer=tf.keras.regularizers.l2(0.04),\n",
        "                                  name='Hidden1'))\n",
        "  \n",
        "  # Implement L2 regularization in the second hidden layer.\n",
        "  model.add(tf.keras.layers.Dense(units=12, \n",
        "                                  activation='relu', \n",
        "                                  kernel_regularizer=tf.keras.regularizers.l2(0.04),\n",
        "                                  name='Hidden2'))\n",
        "    \n",
        "  # Funnel the regression value through a sigmoid function.\n",
        "  model.add(tf.keras.layers.Dense(units=1, input_shape=(1,),\n",
        "                                  activation=tf.sigmoid),)\n",
        "\n",
        "  # Call the compile method to construct the layers into a model that\n",
        "  # TensorFlow can execute.  Notice that we're using a different loss\n",
        "  # function for classification than for regression.    \n",
        "  model.compile(optimizer=tf.keras.optimizers.RMSprop(lr=my_learning_rate),                                                   \n",
        "                loss=tf.keras.losses.BinaryCrossentropy(),\n",
        "                metrics=my_metrics)\n",
        "\n",
        "  return model        \n",
        "\n",
        "\n",
        "def train_model(model, dataset, epochs, label_name,\n",
        "                batch_size=None):\n",
        "  \"\"\"Feed a dataset into the model in order to train it.\"\"\"\n",
        "\n",
        "  # The x parameter of tf.keras.Model.fit can be a list of arrays, where\n",
        "  # each array contains the data for one feature.  Here, we're passing\n",
        "  # every column in the dataset. Note that the feature_layer will filter\n",
        "  # away most of those columns, leaving only the desired columns and their\n",
        "  # representations as features.\n",
        "  features = {name:np.array(value) for name, value in dataset.items()}\n",
        "  label = np.array(features.pop(label_name)) \n",
        "  history = model.fit(x=features, y=label, batch_size=batch_size,\n",
        "                      epochs=epochs)\n",
        "  \n",
        "  # The list of epochs is stored separately from the rest of history.\n",
        "  epochs = history.epoch\n",
        "\n",
        "  # Isolate the classification metric for each epoch.\n",
        "  hist = pd.DataFrame(history.history)\n",
        "\n",
        "  return epochs, hist  \n",
        "\n",
        "print(\"Defined the create_model and train_model functions.\")   "
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defined the create_model and train_model functions.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FPHby_d6TD9o",
        "colab_type": "code",
        "colab": {},
        "outputId": "7235ad9f-b96b-4463-9cbb-009085a72bb6"
      },
      "source": [
        "#@title Define the plotting function.\n",
        "def plot_curve(epochs, hist, list_of_metrics):\n",
        "  \"\"\"Plot a curve of one or more classification metrics vs. epoch.\"\"\"  \n",
        "  # list_of_metrics should be one of the names shown in:\n",
        "  # https://www.tensorflow.org/tutorials/structured_data/imbalanced_data#define_the_model_and_metrics  \n",
        "\n",
        "  plt.figure()\n",
        "  plt.xlabel(\"Epoch\")\n",
        "  plt.ylabel(\"Value\")\n",
        "\n",
        "  for m in list_of_metrics:\n",
        "    x = hist[m]\n",
        "    plt.plot(epochs[1:], x[1:], label=m)\n",
        "\n",
        "  plt.legend()\n",
        "\n",
        "print(\"Defined the plot_curve function.\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Defined the plot_curve function.\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tncEa9m0TD9u",
        "colab_type": "code",
        "colab": {},
        "outputId": "7cbf37af-cbbc-4eff-e55b-8076f70804dc"
      },
      "source": [
        "# The following variables are the hyperparameters.\n",
        "learning_rate = 0.001\n",
        "epochs = 100\n",
        "batch_size = 10\n",
        "label_name = \"Class_att\"\n",
        "classification_threshold = 0.70\n",
        "\n",
        "# Establish the metrics the model will measure.\n",
        "METRICS = [\n",
        "           tf.keras.metrics.BinaryAccuracy(name='accuracy', \n",
        "                                           threshold=classification_threshold),\n",
        "          ]\n",
        "\n",
        "# Establish the model's topography.\n",
        "my_model = create_model(learning_rate, feature_layer, METRICS)\n",
        "\n",
        "# Train the model on the training set.\n",
        "epochs, hist = train_model(my_model, train, epochs, \n",
        "                           label_name, batch_size)\n",
        "\n",
        "# Plot a graph of the metric(s) vs. epochs.\n",
        "list_of_metrics_to_plot = ['accuracy'] \n",
        "\n",
        "plot_curve(epochs, hist, list_of_metrics_to_plot)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Train on 198 samples\n",
            "Epoch 1/100\n",
            "198/198 [==============================] - 2s 11ms/sample - loss: 1.5259 - accuracy: 0.7071\n",
            "Epoch 2/100\n",
            "198/198 [==============================] - 0s 399us/sample - loss: 1.3675 - accuracy: 0.7071\n",
            "Epoch 3/100\n",
            "198/198 [==============================] - 0s 338us/sample - loss: 1.2528 - accuracy: 0.7071\n",
            "Epoch 4/100\n",
            "198/198 [==============================] - 0s 545us/sample - loss: 1.1577 - accuracy: 0.7071\n",
            "Epoch 5/100\n",
            "198/198 [==============================] - 0s 510us/sample - loss: 1.0751 - accuracy: 0.7071\n",
            "Epoch 6/100\n",
            "198/198 [==============================] - 0s 459us/sample - loss: 1.0027 - accuracy: 0.7071\n",
            "Epoch 7/100\n",
            "198/198 [==============================] - 0s 464us/sample - loss: 0.9368 - accuracy: 0.7071\n",
            "Epoch 8/100\n",
            "198/198 [==============================] - 0s 404us/sample - loss: 0.8786 - accuracy: 0.7071\n",
            "Epoch 9/100\n",
            "198/198 [==============================] - 0s 424us/sample - loss: 0.8261 - accuracy: 0.7071\n",
            "Epoch 10/100\n",
            "198/198 [==============================] - 0s 404us/sample - loss: 0.7786 - accuracy: 0.7071\n",
            "Epoch 11/100\n",
            "198/198 [==============================] - 0s 399us/sample - loss: 0.7371 - accuracy: 0.7071\n",
            "Epoch 12/100\n",
            "198/198 [==============================] - 0s 394us/sample - loss: 0.6996 - accuracy: 0.7071\n",
            "Epoch 13/100\n",
            "198/198 [==============================] - 0s 434us/sample - loss: 0.6664 - accuracy: 0.7071\n",
            "Epoch 14/100\n",
            "198/198 [==============================] - 0s 409us/sample - loss: 0.6369 - accuracy: 0.7071\n",
            "Epoch 15/100\n",
            "198/198 [==============================] - 0s 469us/sample - loss: 0.6115 - accuracy: 0.7071\n",
            "Epoch 16/100\n",
            "198/198 [==============================] - 0s 343us/sample - loss: 0.5889 - accuracy: 0.7071\n",
            "Epoch 17/100\n",
            "198/198 [==============================] - 0s 409us/sample - loss: 0.5699 - accuracy: 0.7071\n",
            "Epoch 18/100\n",
            "198/198 [==============================] - 0s 394us/sample - loss: 0.5530 - accuracy: 0.7071\n",
            "Epoch 19/100\n",
            "198/198 [==============================] - 0s 351us/sample - loss: 0.5385 - accuracy: 0.7071\n",
            "Epoch 20/100\n",
            "198/198 [==============================] - 0s 389us/sample - loss: 0.5256 - accuracy: 0.7071\n",
            "Epoch 21/100\n",
            "198/198 [==============================] - 0s 333us/sample - loss: 0.5151 - accuracy: 0.7071\n",
            "Epoch 22/100\n",
            "198/198 [==============================] - 0s 323us/sample - loss: 0.5054 - accuracy: 0.7071\n",
            "Epoch 23/100\n",
            "198/198 [==============================] - 0s 429us/sample - loss: 0.4978 - accuracy: 0.7071\n",
            "Epoch 24/100\n",
            "198/198 [==============================] - 0s 353us/sample - loss: 0.4906 - accuracy: 0.7071\n",
            "Epoch 25/100\n",
            "198/198 [==============================] - 0s 323us/sample - loss: 0.4848 - accuracy: 0.7071\n",
            "Epoch 26/100\n",
            "198/198 [==============================] - 0s 459us/sample - loss: 0.4785 - accuracy: 0.7071\n",
            "Epoch 27/100\n",
            "198/198 [==============================] - 0s 307us/sample - loss: 0.4725 - accuracy: 0.7071\n",
            "Epoch 28/100\n",
            "198/198 [==============================] - 0s 384us/sample - loss: 0.4693 - accuracy: 0.7071\n",
            "Epoch 29/100\n",
            "198/198 [==============================] - 0s 338us/sample - loss: 0.4646 - accuracy: 0.7071\n",
            "Epoch 30/100\n",
            "198/198 [==============================] - 0s 333us/sample - loss: 0.4592 - accuracy: 0.7071\n",
            "Epoch 31/100\n",
            "198/198 [==============================] - 0s 444us/sample - loss: 0.4575 - accuracy: 0.7071\n",
            "Epoch 32/100\n",
            "198/198 [==============================] - 0s 424us/sample - loss: 0.4528 - accuracy: 0.7071\n",
            "Epoch 33/100\n",
            "198/198 [==============================] - 0s 374us/sample - loss: 0.4498 - accuracy: 0.7071\n",
            "Epoch 34/100\n",
            "198/198 [==============================] - 0s 343us/sample - loss: 0.4458 - accuracy: 0.7071\n",
            "Epoch 35/100\n",
            "198/198 [==============================] - 0s 333us/sample - loss: 0.4439 - accuracy: 0.7071\n",
            "Epoch 36/100\n",
            "198/198 [==============================] - 0s 374us/sample - loss: 0.4414 - accuracy: 0.7071\n",
            "Epoch 37/100\n",
            "198/198 [==============================] - 0s 333us/sample - loss: 0.4382 - accuracy: 0.7071\n",
            "Epoch 38/100\n",
            "198/198 [==============================] - 0s 343us/sample - loss: 0.4359 - accuracy: 0.7071\n",
            "Epoch 39/100\n",
            "198/198 [==============================] - 0s 358us/sample - loss: 0.4329 - accuracy: 0.7071\n",
            "Epoch 40/100\n",
            "198/198 [==============================] - 0s 384us/sample - loss: 0.4310 - accuracy: 0.7071\n",
            "Epoch 41/100\n",
            "198/198 [==============================] - 0s 368us/sample - loss: 0.4292 - accuracy: 0.7071\n",
            "Epoch 42/100\n",
            "198/198 [==============================] - 0s 363us/sample - loss: 0.4280 - accuracy: 0.7071\n",
            "Epoch 43/100\n",
            "198/198 [==============================] - 0s 343us/sample - loss: 0.4258 - accuracy: 0.7071\n",
            "Epoch 44/100\n",
            "198/198 [==============================] - 0s 368us/sample - loss: 0.4241 - accuracy: 0.7071\n",
            "Epoch 45/100\n",
            "198/198 [==============================] - 0s 374us/sample - loss: 0.4217 - accuracy: 0.7071\n",
            "Epoch 46/100\n",
            "198/198 [==============================] - 0s 353us/sample - loss: 0.4213 - accuracy: 0.7071\n",
            "Epoch 47/100\n",
            "198/198 [==============================] - 0s 419us/sample - loss: 0.4196 - accuracy: 0.7071\n",
            "Epoch 48/100\n",
            "198/198 [==============================] - 0s 404us/sample - loss: 0.4169 - accuracy: 0.7071\n",
            "Epoch 49/100\n",
            "198/198 [==============================] - 0s 406us/sample - loss: 0.4172 - accuracy: 0.7071\n",
            "Epoch 50/100\n",
            "198/198 [==============================] - 0s 384us/sample - loss: 0.4143 - accuracy: 0.7071\n",
            "Epoch 51/100\n",
            "198/198 [==============================] - 0s 343us/sample - loss: 0.4130 - accuracy: 0.7071\n",
            "Epoch 52/100\n",
            "198/198 [==============================] - 0s 368us/sample - loss: 0.4130 - accuracy: 0.7071\n",
            "Epoch 53/100\n",
            "198/198 [==============================] - 0s 338us/sample - loss: 0.4121 - accuracy: 0.7071\n",
            "Epoch 54/100\n",
            "198/198 [==============================] - 0s 333us/sample - loss: 0.4084 - accuracy: 0.7071\n",
            "Epoch 55/100\n",
            "198/198 [==============================] - 0s 353us/sample - loss: 0.4081 - accuracy: 0.7071\n",
            "Epoch 56/100\n",
            "198/198 [==============================] - 0s 348us/sample - loss: 0.4078 - accuracy: 0.7071\n",
            "Epoch 57/100\n",
            "198/198 [==============================] - 0s 429us/sample - loss: 0.4062 - accuracy: 0.7071\n",
            "Epoch 58/100\n",
            "198/198 [==============================] - 0s 437us/sample - loss: 0.4056 - accuracy: 0.7071\n",
            "Epoch 59/100\n",
            "198/198 [==============================] - 0s 368us/sample - loss: 0.4030 - accuracy: 0.7071\n",
            "Epoch 60/100\n",
            "198/198 [==============================] - 0s 353us/sample - loss: 0.4038 - accuracy: 0.7071\n",
            "Epoch 61/100\n",
            "198/198 [==============================] - 0s 353us/sample - loss: 0.4022 - accuracy: 0.7071\n",
            "Epoch 62/100\n",
            "198/198 [==============================] - 0s 379us/sample - loss: 0.4006 - accuracy: 0.7071\n",
            "Epoch 63/100\n",
            "198/198 [==============================] - 0s 353us/sample - loss: 0.4005 - accuracy: 0.7071\n",
            "Epoch 64/100\n",
            "198/198 [==============================] - 0s 399us/sample - loss: 0.4002 - accuracy: 0.7071\n",
            "Epoch 65/100\n",
            "198/198 [==============================] - 0s 384us/sample - loss: 0.3978 - accuracy: 0.7071\n",
            "Epoch 66/100\n",
            "198/198 [==============================] - 0s 404us/sample - loss: 0.3990 - accuracy: 0.7071\n",
            "Epoch 67/100\n",
            "198/198 [==============================] - 0s 389us/sample - loss: 0.3970 - accuracy: 0.7071\n",
            "Epoch 68/100\n",
            "198/198 [==============================] - 0s 374us/sample - loss: 0.3954 - accuracy: 0.7071\n",
            "Epoch 69/100\n",
            "198/198 [==============================] - 0s 328us/sample - loss: 0.3940 - accuracy: 0.7071\n",
            "Epoch 70/100\n",
            "198/198 [==============================] - 0s 368us/sample - loss: 0.3942 - accuracy: 0.7071\n",
            "Epoch 71/100\n",
            "198/198 [==============================] - 0s 374us/sample - loss: 0.3938 - accuracy: 0.7071\n",
            "Epoch 72/100\n",
            "198/198 [==============================] - 0s 374us/sample - loss: 0.3941 - accuracy: 0.7071\n",
            "Epoch 73/100\n",
            "198/198 [==============================] - 0s 454us/sample - loss: 0.3898 - accuracy: 0.7121\n",
            "Epoch 74/100\n",
            "198/198 [==============================] - 0s 500us/sample - loss: 0.3907 - accuracy: 0.7071\n",
            "Epoch 75/100\n",
            "198/198 [==============================] - 0s 434us/sample - loss: 0.3891 - accuracy: 0.7071\n",
            "Epoch 76/100\n",
            "198/198 [==============================] - 0s 394us/sample - loss: 0.3906 - accuracy: 0.7071\n",
            "Epoch 77/100\n",
            "198/198 [==============================] - 0s 407us/sample - loss: 0.3884 - accuracy: 0.7071\n",
            "Epoch 78/100\n",
            "198/198 [==============================] - 0s 366us/sample - loss: 0.3890 - accuracy: 0.7071\n",
            "Epoch 79/100\n",
            "198/198 [==============================] - 0s 338us/sample - loss: 0.3879 - accuracy: 0.7071\n",
            "Epoch 80/100\n",
            "198/198 [==============================] - 0s 374us/sample - loss: 0.3863 - accuracy: 0.7071\n",
            "Epoch 81/100\n",
            "198/198 [==============================] - 0s 338us/sample - loss: 0.3850 - accuracy: 0.7071\n",
            "Epoch 82/100\n",
            "198/198 [==============================] - 0s 353us/sample - loss: 0.3838 - accuracy: 0.7071\n",
            "Epoch 83/100\n",
            "198/198 [==============================] - 0s 384us/sample - loss: 0.3833 - accuracy: 0.7121\n",
            "Epoch 84/100\n",
            "198/198 [==============================] - 0s 353us/sample - loss: 0.3839 - accuracy: 0.7121\n",
            "Epoch 85/100\n",
            "198/198 [==============================] - 0s 336us/sample - loss: 0.3833 - accuracy: 0.7071\n",
            "Epoch 86/100\n",
            "198/198 [==============================] - 0s 338us/sample - loss: 0.3834 - accuracy: 0.7071\n",
            "Epoch 87/100\n",
            "198/198 [==============================] - 0s 333us/sample - loss: 0.3834 - accuracy: 0.7071\n",
            "Epoch 88/100\n",
            "198/198 [==============================] - 0s 348us/sample - loss: 0.3821 - accuracy: 0.7121\n",
            "Epoch 89/100\n",
            "198/198 [==============================] - 0s 323us/sample - loss: 0.3808 - accuracy: 0.7071\n",
            "Epoch 90/100\n",
            "198/198 [==============================] - 0s 333us/sample - loss: 0.3796 - accuracy: 0.7121\n",
            "Epoch 91/100\n",
            "198/198 [==============================] - 0s 343us/sample - loss: 0.3805 - accuracy: 0.7121\n",
            "Epoch 92/100\n",
            "198/198 [==============================] - 0s 338us/sample - loss: 0.3776 - accuracy: 0.7121\n",
            "Epoch 93/100\n",
            "198/198 [==============================] - 0s 313us/sample - loss: 0.3785 - accuracy: 0.7121\n",
            "Epoch 94/100\n",
            "198/198 [==============================] - 0s 353us/sample - loss: 0.3773 - accuracy: 0.7121\n",
            "Epoch 95/100\n",
            "198/198 [==============================] - 0s 353us/sample - loss: 0.3748 - accuracy: 0.7071\n",
            "Epoch 96/100\n",
            "198/198 [==============================] - 0s 338us/sample - loss: 0.3763 - accuracy: 0.7172\n",
            "Epoch 97/100\n",
            "198/198 [==============================] - 0s 389us/sample - loss: 0.3766 - accuracy: 0.7121\n",
            "Epoch 98/100\n",
            "198/198 [==============================] - 0s 368us/sample - loss: 0.3778 - accuracy: 0.7172\n",
            "Epoch 99/100\n",
            "198/198 [==============================] - 0s 434us/sample - loss: 0.3747 - accuracy: 0.7121\n",
            "Epoch 100/100\n",
            "198/198 [==============================] - 0s 358us/sample - loss: 0.3739 - accuracy: 0.7121\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "efjadbV4TD9x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n5mqRkhoTD90",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SfZ5ZIe_TD96",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7Rf_jJLTD9_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Hosq4Y2tTD-B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6T2zzOuuTD-E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wXzfVy42TD-G",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p-02RdWjTD-J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iO6JT8W0TD-N",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0who868YTD-Q",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zKbZl5K4TD-Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSUx03KbTD-f",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T5sRRx-LTD-i",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sWqhGbAyTD-k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9p1xejQwTD-n",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2VcBD1ggTD-r",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}